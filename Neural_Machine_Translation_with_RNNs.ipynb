{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nimendra-ag/Deep-Learning-NLP-for-Sentiment-analysis-Translation/blob/main/Neural_Machine_Translation_with_RNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wZ-4rFnXb-AB"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf### models\n",
        "import numpy as np### math computations\n",
        "import matplotlib.pyplot as plt### plotting bar chart\n",
        "import sklearn### machine learning library\n",
        "import cv2## image processing\n",
        "from sklearn.metrics import confusion_matrix, roc_curve### metrics\n",
        "import seaborn as sns### visualizations\n",
        "import datetime\n",
        "import pathlib\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "from numpy import random\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.layers import (Dense,Flatten,SimpleRNN,InputLayer,Conv1D,Bidirectional,GRU,LSTM,BatchNormalization,Dropout,Input, Embedding,TextVectorization)\n",
        "from tensorflow.keras.losses import BinaryCrossentropy,CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import Accuracy,TopKCategoricalAccuracy, CategoricalAccuracy, SparseCategoricalAccuracy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "from tensorboard.plugins import projector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiFpjnGSd5mZ"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQkQSOtyd95X"
      },
      "source": [
        "## Data Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Na1NhUXdBti",
        "outputId": "4d118196-6e85-4478-a7a7-ea984977b177"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-03-13 12:21:48--  https://www.manythings.org/anki/fra-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7943074 (7.6M) [application/zip]\n",
            "Saving to: ‘fra-eng.zip’\n",
            "\n",
            "fra-eng.zip         100%[===================>]   7.57M  31.2MB/s    in 0.2s    \n",
            "\n",
            "2025-03-13 12:21:48 (31.2 MB/s) - ‘fra-eng.zip’ saved [7943074/7943074]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.manythings.org/anki/fra-eng.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfLbDeBbdCJK",
        "outputId": "9e22b800-b28d-4432-860d-1048a9f43502"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/fra-eng.zip\n",
            "replace /content/dataset/_about.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "  inflating: /content/dataset/_about.txt  \n",
            "replace /content/dataset/fra.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "  inflating: /content/dataset/fra.txt  \n"
          ]
        }
      ],
      "source": [
        "!unzip \"/content/fra-eng.zip\" -d \"/content/dataset/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ5oTdC4eElF"
      },
      "source": [
        "## Kaggle Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDhBs_7mfkjg"
      },
      "source": [
        "If you want to go with the kaggle data set you can use it. But if you use the other data set don't run these cells which are related to the kaggle dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDA6NqTodJVB",
        "outputId": "c1bad8fa-55f6-47d8-ea38-200756cc0efc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.7.4 / client 1.6.17)\n",
            "Dataset URL: https://www.kaggle.com/datasets/dhruvildave/en-fr-translation-dataset\n",
            "License(s): ODbL-1.0\n",
            "Downloading en-fr-translation-dataset.zip to /content\n",
            "100% 2.54G/2.54G [00:30<00:00, 122MB/s] \n",
            "100% 2.54G/2.54G [00:30<00:00, 90.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d dhruvildave/en-fr-translation-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ne8DbxqAeLY5",
        "outputId": "92becdee-dbea-4ce1-afda-7f9e15898f0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/en-fr-translation-dataset.zip\n",
            "  inflating: /content/dataset/en-fr.csv  \n"
          ]
        }
      ],
      "source": [
        "!unzip \"/content/en-fr-translation-dataset.zip\" -d \"/content/dataset/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvDAFe37eWeg"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.experimental.CsvDataset(\n",
        "  \"/content/dataset/en-fr.csv\",\n",
        "  [\n",
        "    tf.string,\n",
        "    tf.string\n",
        "  ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EB1acAy6eZM1"
      },
      "source": [
        "## Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FzDWAAdyeYNQ"
      },
      "outputs": [],
      "source": [
        "text_dataset=tf.data.TextLineDataset(\"/content/dataset/fra.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DR1j8hKSekLC"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE=20000\n",
        "ENGLISH_SEQUENCE_LENGTH=64\n",
        "FRENCH_SEQUENCE_LENGTH=64\n",
        "EMBEDDING_DIM=300\n",
        "BATCH_SIZE=64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gHZdACIzero7"
      },
      "outputs": [],
      "source": [
        "english_vectorize_layer=TextVectorization(\n",
        "    standardize='lower_and_strip_punctuation',\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=ENGLISH_SEQUENCE_LENGTH\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "french_vectorize_layer=TextVectorization(\n",
        "    standardize='lower_and_strip_punctuation',\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=FRENCH_SEQUENCE_LENGTH\n",
        ")"
      ],
      "metadata": {
        "id": "YPR6cjipuySz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def selector(input_text):\n",
        "  split_text=tf.strings.split(input_text,'\\t')\n",
        "  return {'input_1':split_text[0:1],'input_2':'starttoken '+split_text[1:2]},split_text[1:2]+' endtoken'"
      ],
      "metadata": {
        "id": "jnhr2bJNy8K5"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_dataset=text_dataset.map(selector)"
      ],
      "metadata": {
        "id": "tVZed1qLziLf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def separator(input_text):\n",
        "  split_text=tf.strings.split(input_text, '\\t')\n",
        "  return split_text[0:1], 'starttoken' + split_text[1:2] + ' endtoken'"
      ],
      "metadata": {
        "id": "5OdkrhWK2LDA"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in split_dataset.take(3):\n",
        "  print(i[0]['input_1'].numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lsqLN772eZ4",
        "outputId": "30c79399-60f1-4591-a7e9-ed1d4653f55a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[b'Go.']\n",
            "[b'Go.']\n",
            "[b'Go.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "init_dataset=text_dataset.map(separator)"
      ],
      "metadata": {
        "id": "TeqGKe0G2m-v"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in init_dataset.take(3):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjaT1-N84bT3",
        "outputId": "c8b39690-91b7-4ff7-ce87-2607598e3a36"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttokenVa ! endtoken'], dtype=object)>)\n",
            "(<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttokenMarche. endtoken'], dtype=object)>)\n",
            "(<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttokenEn route ! endtoken'], dtype=object)>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "english_training_data=init_dataset.map(lambda x,y:x)  #input x, y and output x\n",
        "english_vectorize_layer.adapt(english_training_data)"
      ],
      "metadata": {
        "id": "T08plaA34pUA"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "french_training_data=init_dataset.map(lambda x,y:y) #input x, y and output y\n",
        "french_vectorize_layer.adapt(french_training_data)"
      ],
      "metadata": {
        "id": "_cSa9ATR5kqF"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(english_vectorize_layer.get_vocabulary()))\n",
        "print(len(french_vectorize_layer.get_vocabulary()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-rWtX2W5TQT",
        "outputId": "9b621fd9-2135-4df2-c339-8cf99cf11dd9"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16952\n",
            "20000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorizer(inputs, output):\n",
        "  return {'input_1':english_vectorize_layer(inputs['input_1']),\n",
        "          'input_2':french_vectorize_layer(inputs['input_2'])},french_vectorize_layer(output)"
      ],
      "metadata": {
        "id": "b96Rxgu657-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXdKWUul_FCt",
        "outputId": "1df6677f-1e44-4d95-a0af-729bf6d0220d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_MapDataset element_spec=({'input_1': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'input_2': TensorSpec(shape=(None,), dtype=tf.string, name=None)}, TensorSpec(shape=(None,), dtype=tf.string, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H2sxfNVi_GZH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjCLvWRb2FqrjY8DWwn70X",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}